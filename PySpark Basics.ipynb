{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary (Part I)\n",
    "## The conceptual model of Spark and the RDD distributed compute model\n",
    "## The use of SparkContext and SparkSession to initiate Spark instances on a cluster\n",
    "## Transformations, actions and where Spark performs lazy evaluation\n",
    "## How to create, query and manipulate Spark SQL DataFrames\n",
    "## Type conversion and filtering of Spark SQL DataFrames\n",
    "## How to perform Exploratory Data Analysis (EDA) on large datasets using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Session and Context\n",
    "#Spark Session - maintains information about our connection with the cluster\n",
    "#Components within the Spark Application : Driver, Master, the Cluster Manager, and the Executors(s), Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext: represents a connection to the remote or local spark cluster, and was the main entry point for earlier versions of Spark\n",
    "# spark context - older style\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark session - hich has evolved to include all the interface options from the spark context. You should use the spark session as your entry point moving forward.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = \\\n",
    "SparkSession.builder.appName('spark_training').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End a sparkSession\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "#Import csv\n",
    "df_person = spark.read.csv('person_demo.csv', header=True,\n",
    "                           inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine object\n",
    "df_person\n",
    "#type of data\n",
    "type(df_person)\n",
    "#examine the first line\n",
    "df_person.first()\n",
    "#examine the first three lines\n",
    "df_person.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n",
    "df_person.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the data\n",
    "df_person.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 100 rows to pandas (DataFrame) and visualize\n",
    "#be aware that this means we will collect all the data to the driver node, so use this option only with small datasets (or reduce or aggregate your datasets accordingly).\n",
    "df_person.limit(100).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An RDD is the core data object in Spark\n",
    "#Spark DataFrames\n",
    "n_cols = 5\n",
    "n_rows = 10\n",
    "pandas_df = pd.DataFrame(\n",
    "{chr(x + 65): range(x, x + n_rows) for x in range(n_cols)}, dtype=np.float64)\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark DataFrame from Pandas\n",
    "spark_df.head(5)\n",
    "spark_df.columns\n",
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schemas\n",
    "#Create pandas dataframe\n",
    "#Specify the data types\n",
    "df_person.printSchema()\n",
    "\n",
    "import pyspark.sql.types as st\n",
    "schema1 = st.StructType([st.StructField(\"Clust_id\", st.StringType(),True),\n",
    "st.StructField(\"p_id\", st.IntegerType(),True), st.StructField(\"Age\", st.FloatType(),True), st.StructField(\"Baseline_date\", st.StringType(),True), st.StructField(\"Gender\", st.StringType(),True), st.StructField(\"Income\", st.FloatType(),True), st.StructField(\"Education\", st.StringType(),True), st.StructField(\"lab_value_1\", st.FloatType(),True), st.StructField(\"lab_value_2\", st.FloatType(),True), st.StructField(\"lab_value_3\", st.FloatType(),True), st.StructField(\"outcome\", st.StringType(),True)])\n",
    "# create the dataframe\n",
    "df2_spark = spark.createDataFrame(df2_pd,schema=schema1) ## no error now!\n",
    "# Examine result df2_spark df2_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL\n",
    "#The sparkSession is the main entry point for Spark SQL applications\n",
    "#Typical Data Management\n",
    "df_person.printSchema()\n",
    "df_person.count()\n",
    "df_person.describe().show()\n",
    "df_person[[\"Age\",\"p_id\"]].show(5)\n",
    "df_person.select('Age','p_id').show(5)\n",
    "df_person.filter(df_person['Age'] > 90).show(5)\n",
    "df_person.filter(df_person['Age']> 90).describe('Age').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Column Types\n",
    "import pyspark.sql.functions as s_f\n",
    "df_person = df_person.withColumn('Baseline_date_dt',\n",
    "     s_f.to_date(df_person['Baseline_date'], 'MM/dd/yyyy'))\n",
    "\n",
    "df_person.select('Baseline_date','Baseline_date_dt').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new variables as function of others\n",
    "df_person =\n",
    "df_person.withColumn('log_income',\n",
    "                s_f.log(df_person['Income'])) df_person.select('Income','log_income').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender variable example\n",
    "df_person.select('Gender').distinct().show()\n",
    "\n",
    "df_person.groupBy('Gender').count().show()\n",
    "\n",
    "df_person = df_person.withColumn('Gender',\n",
    "                                    s_f.when(df_person['Gender'] != '999',\n",
    "                                            df_person['Gender']).otherwise(None))\n",
    "\n",
    "df_person.groupBy('Gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics: Quantiles\n",
    "#Calculate quantiles for Age\n",
    "df_person.select('Age').dropna().approxQuantile('Age',[0.5],0)\n",
    "\n",
    "df_person.select('Age').dropna().approxQuantile('Age',[0.25,0.5,0.75],0)\n",
    "\n",
    "df_person.select('Age').describe().show()\n",
    "\n",
    "df_person = df_person.withColumn('Age', s_f.when(\n",
    "                            df_person['Age'] < 100,df_person['Age']).otherwise(None))\n",
    "\n",
    "df_person.select('Age').describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By Analysis\n",
    "df_person.groupBy('Gender').avg('Income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By Multiple columns\n",
    "df_person.groupBy('Gender','Education').avg('Income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot (create a pivot table with col and row variables)\n",
    "df_person.groupBy('Gender').pivot('Education').avg('Income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation\n",
    "#Currently, only the Pearson correlation coefficient is supported\n",
    "df_person.corr('Age','Income') # Pearson correlation coef\n",
    "\n",
    "df_person.corr(\"Age\",\"lab_value_1\") # Pearson\n",
    "correlation coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL Usage\n",
    "#A convenient entry-point is to register your dataset as a table\n",
    "#We can now interact with the table using SQL queries\n",
    "df_person.createOrReplaceTempView('person')\n",
    "\n",
    "spark.sql('select p_id, age from person where age > 55').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select corr(age,income) as cor from person where age > 55').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select min(age) as min, max(age) as max from person where age > 55').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Data\n",
    "#We can merge data using a Pandas-like .join() method\n",
    "#Here we load another dataset in order to merge it to the first\n",
    "\n",
    "#.join() method\n",
    "#create spark dataframe\n",
    "df_cluster = spark.read.csv('cluster_demo.csv',header=True,inferSchema=True)\n",
    "df_cluster.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join \n",
    "df_left = df_person.join(df_cluster, on='Clust_id', how = 'left_outer')\n",
    "\n",
    "#right join\n",
    "df_right = df_person.join(df_cluster, on='Clust_id', how = 'right_outer')\n",
    "\n",
    "#inner join\n",
    "df_inner = df_person.join(df_cluster, on = 'Clust_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part II - PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform machine learning within the Spark environment, using the Spark ML API from the PySpark Python library\n",
    "## How to perform regression within Spark\n",
    "## Various classification routines\n",
    "## How to construct a multi-layer perceptron (MLP) feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading library\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "#To generate labels column from categorical data\n",
    "from pyspark.ml.feature \n",
    "import OneHotEncoderEstimator, StringIndexer\n",
    "\n",
    "#To generate features vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark_ml').getOrCreate() DATA_DIR = \"../data\" #or wherever this might be\n",
    "\n",
    "cnc_data = spark.read.csv(os.path.join(DATA_DIR, \"manufacturing\", \"CNC_Tool_Wear\", \"cnc_experiments_all.csv\"),\n",
    "header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnc_class_labels = StringIndexer(inputCol='tool_condition',outputCol='label')\n",
    "\n",
    "cnc_data = cnc_class_labels.fit(cnc_data).transform(cnc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Features\n",
    "## convert the non-string features to a feature vector\n",
    "cnc_feature_labels = cnc_data.columns[0:47] + cnc_data.columns[49:51]\n",
    "\n",
    "cnc_feature_vector = VectorAssembler(\n",
    "    inputCols=cnc_feature_labels,\n",
    "    outputCol=\"features\")  # Note the strict naming convention here\n",
    "\n",
    "cnc_data = cnc_feature_vector.transform(cnc_data)\n",
    "\n",
    "print(\"\\nConverted features and labels for CNC wear dataset:\")\n",
    "\n",
    "cnc_data.sample(0.001).select(\"features\",\"label\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data\n",
    "cnc_train, cnc_test = cnc_data.randomSplit([0.75, 0.25], 42)\n",
    "\n",
    "print(f\"\\nRows of data for training: {cnc_train.count()}, testing: {cnc_test.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting a Model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logReg = LogisticRegression(maxIter=10, regParam=0.3,elasticNetParam=0.8)\n",
    "logRegModel = logReg.fit(cnc_train)\n",
    "\n",
    "print(f\"Coefficients: {logRegModel.coefficients}; Intercept: {logRegModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating a model\n",
    "##logistic regression classifier\n",
    "##elastic net\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "validation = logRegModel.transform(cnc_test).select(\"rawPrediction\", \"label\") \n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\") \n",
    "\n",
    "print(f\"\\nValidation accuracy: {evaluator.evaluate(validation)*100}%\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "concrete_data = spark.read.csv(os.path.join(DATA_DIR, \n",
    "                                            \"manufacturing\",\"concrete\",\"concrete.csv\"),\n",
    "                                            header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Data\n",
    "#We rename the last column (compressive strength) to 'label':\n",
    "#We would like to make the rest into a feature vector, and the rest of the\n",
    "#logic is similar to that of the logistic regression example above. \n",
    "#Once the logic is clear, we may wish to formalize this as a pipeline. \n",
    "\n",
    "#This will apply each of the data transforms and model fits into a single operation.\n",
    "concrete_data = concrete_data.withColumnRenamed(concrete_data.columns[-1], \"label\")\n",
    "concrete_feature_labels = concrete_data.columns[0:8]\n",
    "concrete_feature_vector = VectorAssembler(\n",
    "                                        inputCols=concrete_feature_labels,\n",
    "                                        outputCol=\"features\")  # Note the strict naming convention here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data\n",
    "concrete_train, concrete_test = concrete_data.randomSplit([0.75, 0.25], 42)\n",
    "print(f\"\\nRows of data for training:\n",
    "                               {concrete_train.count()}, testing:\n",
    "                               {concrete_test.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the Model\n",
    "# instantiate\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "linReg = LinearRegression(\n",
    "                maxIter=10, regParam=0.3, elasticNetParam=0.8, solver='normal')\n",
    "# fit\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "lin_pipeline = Pipeline(stages=[concrete_feature_vector, linReg]) \n",
    "linRegModel = lin_pipeline.fit(concrete_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients and Intercept\n",
    "print(f\"Coefficients: {linRegModel.stages[-1].coefficients}\")\n",
    "\n",
    "print(f\"Intercept: {linRegModel.stages[-1].intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the Model\n",
    "trainingSummary = linRegModel.stages[-1].summary\n",
    "\n",
    "print(f\"numIterations:{trainingSummary.totalIterations}\")\n",
    "print(f\"objectiveHistory: {trainingSummary.objectiveHistory}\" )\n",
    "\n",
    "trainingSummary.residuals.show(10)\n",
    "\n",
    "print(f\"RMSE: {trainingSummary.rootMeanSquaredErr or}\")\n",
    "print(f\"r2: {trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Evaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "concrete_test = concrete_feature_vector.transform(concrete_test)\n",
    "validation = linRegModel.stages[- 1].transform(concrete_test).select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "print(f\"\\nRMSE: {evaluator.evaluate(validation)}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "#Tuning library in Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Validation Split\n",
    "## One possibility is to split a single validation set at random from the training data and select the best model, \n",
    "## from a given range of hyperparameters (grid search), according to some evaluation metric. \n",
    "## This is the purpose of TrainValidationSplit:\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(linReg.regParam, [0.1, 0.01, 5.0])\\ \n",
    "        .addGrid(linReg.elasticNetParam, np.arange(0.0, 1.25, 0.25))\\ \n",
    "        .addGrid(linReg.fitIntercept, [False, True])\\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we will split off 25% of the training data to a single validation set. \n",
    "#We'll make use of the linReg linear regressor and RegressionEvaluator we created above (which has an RMSE default output metric):\n",
    "#We can then fit the best model, according to the evaluator, by exploring the hyperparameter grid:\n",
    "trainValSplit = TrainValidationSplit(estimator=linReg, estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.75)\n",
    "concrete_train = concrete_feature_vector.transform(concrete_train)\n",
    "linReg_tvs = trainValSplit.fit(concrete_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default RegressionEvaluator requires a prediction and a label column:\n",
    "# We can see what coefficients were chosen\n",
    "linReg_valid = linReg_tvs.transform(concrete_test).select(\"prediction\", \"label\") \n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "print(f\"\\nRMSE: {evaluator.evaluate(linReg_valid)}\\n\" )\n",
    "\n",
    "print(f\"Model coefficients: {linReg_tvs.bestModel.coefficients}\") \n",
    "print(f\"Model intercept: {linReg_tvs.bestModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross Validation\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "crossValid = CrossValidator(estimator=linReg,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=RegressionEvaluator(),\n",
    "                              numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have set 𝑘=5 and recycled the paramGrid, estimator and evaluator from above. \n",
    "# This may take a while to train, depending on the parameter grid size and number of folds.\n",
    "linReg_cv = crossValid.fit(concrete_train) \n",
    "linReg_cv.transform(concrete_test).select(\"features\", \"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning with Spark\n",
    "## Spark ML has a Multilayer Perceptron (MLP) built-in function. \n",
    "## This is a feed-forward artificial neural network, used in many classification problems. \n",
    "## It is a very powerful classifier and is relatively simple to implement in Spark. \n",
    "## However, non-trivial models require a lot of computational power to train.\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_architecture = [len(cnc_feature_labels), 128, 128, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate and train the model (fit)\n",
    "mlp_template = MultilayerPerceptronClassifier( \n",
    "                                            maxIter=100,\n",
    "                                            layers=mlp_architecture, \n",
    "                                            blockSize=128, \n",
    "                                            seed=13579)\n",
    "mlp_model = mlp_template.fit(cnc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate against the test data\n",
    "validation = mlp_model.transform(cnc_test).select(\"rawPrediction\", \"label\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "print(f\"\\nValidation accuracy: {evaluator.evaluate(validation)*100}%\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
